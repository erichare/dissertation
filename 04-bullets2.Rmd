\chapter{A MODERN BULLET MATCHING APPLICATION}

\begin{center}

A paper to be submitted to the \textbf{Journal of Forensic Science}. \\
Eric Hare, Heike Hofmann, Alicia Carriquiry

\textbf{Abstract}

\end{center}

Bullet matching is a process used to determine whether two bullets have been fired from the same gun barrel. While traditionally a manual process performed by trained forensic examiners, recent work has been done to add statistical validity and objectivity to the procedure. In this paper, we build upon the algorithms explored in Automatic Matching of Bullet Lands by describing a database structure which tracks every parameter used, and allows for the seamless replication of results by researchers, adding a layer of reproducibility to the bullet matching process. Finally, we describe a front-end web application which utilizes the algorithms and the database to allow bullet matching to be done more efficiently.

\newpage

# Background

The need for advancements in terms of scientific objectivity and reproducibility of forensic methods is well known. Note, for example, the recent report by the President's Council of Advisors on Science on Technology (PCAST) [@pcast2016]. The report references a number of areas of common practice in the field in which subjectivity is far too common, including but not limited to fingerprint analysis, bitemark analysis and firearms analysis.

Work has been done to address these concerns and add objectivity to these procedures. In the case of firearms analysis, the focus of this paper, some examples include @vorburger:2011, @thompson:2013, and @riva:2014. Our own work, "Automatic Matching of Bullet Lands", describes procedures used to produce an estimate of the probability of a match between two bullet lands. It does so by deriving a number of features, some from the literature and some original, and computing these features on pairs of reference bullets from the NIST Ballistics Toolmark Research Database (NBTRD). The algorithms used are published as open-source R code available in the package `bulletr` [@bulletr]. In spite of these steps towards transparency, however, the process of duplicated and assessing the performance of the algorithm in hopes of improving predictive accuracy was cumbersome in a number of ways:

1. Doing so requires specialized statistical software (specifically, R and associated R packages)
2. Computing statistics on all pairs of bullet lands is a time consuming process even on high-powered machines (on the order of several days)
3. Updates to our `bulletr` package, or any package dependencies of `bulletr`, may change the results such that our findings are not completely reproducible even if each step is correctly followed

In this paper, we add a new layer of reproducibility to the algorithms to allow for forensic scientists, statisticians, and other interested researchers to duplicate and iterate on the results in a seamless fashion. We do so by introducing a new database structure that supplements the NIST database by storing all necessary parameters and intermediate results needed to arrive at a matching probability between two bullet lands. We describe the structure from a technical perspective, and then describe the front-end and back-end application structure which utilizes the database to provide results to the researchers. Finally, we provide a case study analysis on features of bullet land pairs as an example of the capabiilities of this structure when it is leveraged.

# Database Structure

Figure \ref{fig:database_schema} displays the database schema along with links between the relevant id columns. This structure provides the necessary links between the raw input data, and the processed signatures used to compute features and ultimately provide matching probabilities between pairs of lands. This diagram will be explained in depth in this section.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{images/schema.png}
\caption{A schematic of the database.}
\label{fig:database_schema}
\end{figure}

We can connect to the database using the `dplyr` package.

```{r, message=FALSE, cache=FALSE}
library(xtable)
library(RMySQL)
library(dplyr)
library(tidyr)
library(ggplot2)
library(randomForest)
library(caret)
library(purrr)

dbname <- "bullets"
user <- "buser"
password <- readLines("buser_pass.txt")
host <- "50.81.214.252"
port <- 3306

my_db <- src_mysql(dbname, host, port, user, password)
```

The remainder of this section will walk through the most relevant database tables, and include reproducible R code for accessing, parsing, and displaying the data.

## Data

The data table is essentially a mirror of the bullets stored in the NBTRD. It currently includes a long-form version of the two Hamby bullet sets (Set 252 and Set 44) [@hamby:2009] and the Cary Persistence study [@cary]. A sample of 20 rows of this table can be seen in Table \ref{tab:data}. The land id column identifies the bullet land under consideration. The x coordinate is the location along the shorter axis, while the y coordinate is along the longer axis. The value column represents the height of the bullet at that particular location.

```{r}
my_data <- tbl(my_db, "data")

# Get Hamby Barrel 1 Bullet 1 Land 3
result <- my_data %>% 
    filter(land_id == 39, !is.na(value)) %>%
    arrange(x, y) %>%
    head(n = 20) %>%
    as.data.frame
```

```{r, echo=FALSE, message=FALSE, results='asis'}
print(xtable(result, digits = c(0, 0, 0, 0, 2, 2), label = "tab:data", caption = "A sample of 20 rows of the data table. The land id column identifies the bullet land under consideration. The x coordinate is the location along the shorter axis, while the y coordinate is along the longer axis. The value column represents the height of the bullet at that particular location."), comment = FALSE)
```

In comparison to a regular x3p file, this data table is less space efficient. An x3p file uses a surface matrix of dimension $(x, y)$ where the value of each $(x_i, y_j)$ is the height of the bullet at $x = i$ and $y = j$. Our expanded version of the format turns each cell into a single row $x_i, y_j, z_{ij}$. Where $z_{ij}$ where $z_{ij}$ is the height of the bullet at $x = i$, $y = j$. Thus, for a bullet with 500 $x$ values and 1572 $y$ values (as in land id 39), an x3p file uses a matrix of size 786000, while we use a data frame of 786000 rows and 3 columns, storing $3x$ as much information. While certainly less space efficient, the format allows for easy querying of specific lands or specific profiles from the database.

## Metadata

As shown in Table \ref{tab:metadata}, the metadata table includes one row for each unique bullet land. The name of the bullet is derived from the file path, as provided by the NBTRD. Several other parameters are given, including the number of profiles, (x values), number of observations per profile (y values), and the increments of each in micrometers. (in this case, one x unit is equivalent to 1.5625 micrometers). Note that this table includes parameters that are derived solely from the properties of the data itself. Hence, the previous data table in conjunction with metadata forms the information needed to generate x3p files. Conversely, x3p files can be used to regenerate the data contained in both these tables.

```{r}
my_metadata <- tbl(my_db, "metadata")

result <- my_metadata %>% 
    filter(land_id >= 61, land_id <= 72) %>%
    as.data.frame
```

```{r, echo=FALSE, results='asis'}
print(xtable(result, digits = c(0, 0, 0, 0, 0, 0, 0, 0, 4, 4), label = "tab:metadata", caption = "A sample of 12 rows of the metadata table. The land id column identifies the bullet land under consideration. The name of the bullet is derived from the file path, as provided by the NBTRD. Several other parameters are given, including the number of profiles, (x values), number of observations per profile (y values), and the increments of each in micrometers. (in this case, one x unit is equivalent to 1.5625 micrometers)"), comment = FALSE, include.rownames = FALSE)
```

## Metadata Derived

Similarly to the metadata table, the metadata_derived table (Sampled in Table \ref{tab:metadata_derived}) includes one row per bullet land. The difference is that the columns of this table were derived by our algorithm rather than properties of the data. The run_id, which will be discussed in more depth later, indicates the algorithm run that yielded the following derived parameters.

```{r}
my_metadata_derived <- tbl(my_db, "metadata_derived")

result <- my_metadata_derived %>% 
    filter(land_id >= 61, land_id <= 72) %>%
    as.data.frame
```

```{r, echo=FALSE, results='asis'}
print(xtable(result, digits = c(0, 0, 0, 0, 4, 4, 0, 0), label = "tab:metadata_derived", caption = "A sample of 12 rows of the metadata\\_derived table. Once again, a land id identifies a particular bullet land. The run id, which will be discussed in more depth later, indicates the algorithm run that yielded the following derived parameters."), comment = FALSE, include.rownames = FALSE)
```

A number of derived parameters are given for a particular land and a particular run:

1. **ideal_crosscut** - The location of the ideal cross section (or ideal x coordinate) at which to extract a profile, as given by Hare, Hofmann, Carriquiry (2017).
2. **left_twist** - The calculated twist of the scan as determined by the left shoulder.
3. **right_twist** - The calculated twist of the scan as determined by the right shoulder.
4. **left_sample** - The number of samples used to compute the left twist.
5. **right_sample** - The number of samples used to compute the right twist.

## Profiles

Table \ref{tab:profiles} displays 10 profiles from land id 39, using the first 20 x coordinate values. Profiles are defined by properties of the grooves or shoulders. Hence, given this information, we can post-process the data table to extract particular profiles. For instance, we can use profile_id 32448, which is the profile obtained by extracting at x = 100 for land_id 39. Using properties of the grooves as determined by our algorithm, we can extract the the profile with the shoulders and grooves removed.

```{r}
my_profiles <- tbl(my_db, "profiles")

result <- my_profiles %>% 
    filter(land_id == 39, x > 92, x < 107) %>%
    select(-groove_left_pred, -groove_right_pred) %>%
    as.data.frame
```

```{r, echo=FALSE, results='asis'}
print(xtable(result, digits = c(0, 0, 0, 0, 2, 2, 2), label = "tab:profiles", caption = "A sample of 10 rows of the profiles table. A profile id is uniquely identified by a land id, a run id, and an x value."), comment = FALSE, include.rownames = FALSE)
```

Figure \ref{fig:prof} displays the profile obtained by extracting land id 39 at x = 100. Dashed vertical lines indicate the location of the shoulders. Within the bounds of the dashed line, the profiles that are relevant for bullet matching are obtained.

```{r prof, message=FALSE, warning=FALSE, fig.height=3, fig.width=6, fig.cap='The profile obtained by extracting land id 39 at x = 100. Dashed vertical lines indicate the location of the shoulders. Within the bounds of the dashed line, the profiles that are relevant for bullet matching are obtained.'}
myprof <- filter(result, x == 100)

land39 <- my_data %>% 
    filter(land_id == 39, x == 100) %>%
    as.data.frame

ggplot(data = land39, aes(x = y, y = value)) +
    geom_line() +
    geom_vline(xintercept = myprof$groove_left, linetype = 2) +
    geom_vline(xintercept = myprof$groove_right, linetype = 2) +
    theme_bw()
```

## Signatures

The land signature represents the processed data that is ultimately used for matching. In our case, a land signature represents the smoothed and processed residuals obtained from fitting a Locally Weighted Scatterplot Smoothing Regression (LOESS) to the profiles from above. Figure \ref{fig:sigs} displays the signature obtained by processing the profile of land id 39 at x = 100. It can be seen that the signature represents an attempt at reducing a bullet land to the peaks and valleys that represent striations, by removing the global structure of the bullet that dominates the view of the profile.

```{r sigs, message=FALSE, warning=FALSE, fig.height=3, fig.width=6, fig.cap='The signature obtained by processing the profile of land id 39 at x = 100.'}
my_signatures <- tbl(my_db, "signatures")

result <- my_signatures %>% 
    filter(profile_id == myprof$profile_id, run_id == 1) %>%
    as.data.frame

ggplot(data = result, aes(x = y, y = l30)) +
    geom_line() +
    theme_bw()
```

## CCF

The CCF table contains features computed on cross-comparisons between different signatures. The name is a bit of a misnomer; the ccf, or cross-correlation function, is only one of the features.  Table \ref{tab:ccf} displays the features given by this table for a comparison of land id 39, from above, with its match, land id 47.

```{r}
my_ccf <- tbl(my_db, "ccf")

metadata2 <- my_metadata_derived %>%
    filter(land_id == 47) %>%
    as.data.frame

myprof2 <- my_profiles %>% 
    filter(land_id == 47, x == metadata2$ideal_crosscut) %>%
    select(-groove_left_pred, -groove_right_pred) %>%
    as.data.frame

result <- my_ccf %>% 
    filter(profile1_id == myprof$profile_id, 
           profile2_id == myprof2$profile_id, 
           compare_id == 3) %>%
    as.data.frame %>%
    select(-compare_id, -profile1_id, -profile2_id) %>%
    gather(key = Feature, value = Value) %>%
    mutate(Value = round(Value, digits = 4)) %>%
    as.data.frame
```

```{r, echo=FALSE, results='asis'}
print(xtable(result, digits = c(0, 4, 4), label = "tab:ccf", caption = "The derived features, given by the ccf table, for a comparison of land id 39 and land id 47."), comment = FALSE, include.rownames = FALSE)
```

# Case Study: Feature Analysis

With the database structure defined, we turn our attention to using the database to perform a feature analysis using the bullets provided in the NBTRD. The modular structure of the database allows us to extract pre-derived features without performing the intermediate computations, which in turn allows for easier in depth exploration.

To start, we introduce each feature provided in the CCF database table. Table \ref{tab:ccf1} provides the first eight columns of the CCF table, and Table \ref{tab:ccf2} provides the remaining seven columns.

```{r, echo=FALSE, results='asis'}
result <- my_ccf %>%
    filter(compare_id == 3) %>%
    head %>%
    collect %>%
    select(1:8) %>%
    as.data.frame

print(xtable(result, digits = c(0, 0, 0, 0, 4, 4, 4, 4, 4), label = "tab:ccf1", caption = "A sample of six land-to-land comparisons, and derived features for these comparisons."), comment = FALSE, include.rownames = FALSE)
```

```{r, echo=FALSE, results='asis'}
result2 <- my_ccf %>%
    filter(compare_id == 3) %>%
    head %>%
    collect %>%
    select(9:ncol(.)) %>%
    as.data.frame

print(xtable(result2, digits = c(4, 4, 4, 4, 4, 4, 4, 4), label = "tab:ccf2", caption = "The remaining derived features for the previous six land-to-land comparisons."), comment = FALSE, include.rownames = FALSE)
```

The `compare_id`, `profile1_id`, and `profile2_id` uniquely identify a particular cross-comparison between two profiles, with the given `compare_id` parameters. The remaining columns are the relevant features for the comparison. The definitions of the features have been generalized to account for the possibility of handling degraded bullet conditions, where only fragments of lands can be recovered. The definitions of each are given below, where $f(t)$ represents the height values of the first profile, and $g(t)$ the height values of the second:

- *ccf* (%) is the maximum value of the Cross-Correlation function evaluated at the optimal alignment. The Cross-Correlation function is defined as $C(\tau) = \int_{-\infty}^{\infty} f(t)g(t + \tau)dt$ where $\tau$ represents the the lag of the second signature.
- *double_cor* (%) is a new feature representing the correlation between the two signatures after performing a second LOESS smoothing stage and subtracting the result from the original signatures. This attempts to model the roughness of the surface after removing the waviness.
- *lag* (millimeters) Is the optimal lag for the ccf value.
- *D* is the Euclidean vertical distance between each height value of the aligned signatures. This is defined as $D^2 = \frac{1}{\text{\#}t}\sum_t \left[f(t) - g(t)\right]^2$
- *sd_D* provides the standard deviation of the values of *D* from above.
- *signature_length* (millimeters) is the overall length of the smallest of the two aligned signatures.
- *overlap* (%) provides the percentage of the two signatures that overlap after the alignment stage.
- *matches* (millimeters) is the number of matching peaks/valleys (striae) per millimeter of the overlapping portion of the aligned signatures.
- *mismatches* (millimeters) is the number of mismatching peaks/valleys (striae) per millimeter of the overlapping portion of the aligned signatures.
- *cms* (millimeters) is the number of consecutively matching peaks/valleys (striae) per millimeter of the overlapping portion of the aligned signatures.
- *non_cms* (millimeters) is the number of consecutive mismatching peaks/valleys (striae) per millimeter of the overlapping portion of the aligned signatures.
- *sumpeaks* (millimeters) is the the sum of the average heights of matched striae.

Using these features the randomForest [@randomForest] package, we can train a model which attempts to predict whether two lands match given the values of these features. We first join information on the study the data originated from, along with information on whether they are matches. The three studies currently contained in the database are Hamby (Set 252), Hamby (Set 44), and Cary.

```{r, cache=FALSE}
con <- dbConnect(MySQL(), user = user, password = password,
                 dbname = dbname, host = host)

all_bullets_metadata <- dbReadTable(con, "metadata")
my_matches <- dbReadTable(con, "matches")
profiles <- dbReadTable(con, "profiles")
ccf <- dbReadTable(con, "ccf") %>% filter(compare_id == max(compare_id))

CCFs_withlands <- ccf %>%
    left_join(dplyr::select(profiles, profile_id, land_id), by = c("profile1_id" = "profile_id")) %>%
    left_join(dplyr::select(profiles, profile_id, land_id), by = c("profile2_id" = "profile_id")) %>%
    left_join(my_matches, by = c("land_id.x" = "land1_id", "land_id.y" = "land2_id")) %>%
    left_join(dplyr::select(all_bullets_metadata, land_id, study, barrel, bullet, land), by = c("land_id.x" = "land_id")) %>%
    left_join(dplyr::select(all_bullets_metadata, land_id, study, barrel, bullet, land), by = c("land_id.y" = "land_id")) %>%
    mutate(match = as.logical(match)) 
```

With this data, we can now train the forest. We split the data into an 80% training, 20% testing framework to assess its out of sample performance.

```{r}
includes <- setdiff(names(CCFs_withlands), c("compare_id", "profile1_id", "profile2_id",
                                         "study.x", "study.y", "barrel.x", "barrel.y",
                                         "bullet.x", "bullet.y", "land.x", "land.y",
                                         "land_id.x", "land_id.y"))

set.seed(20170222)
trainIndex <- createDataPartition(1:nrow(CCFs_withlands), p = 0.8, list = TRUE, times = 10)
match_result <- trainIndex %>% purrr::map_df(function(index) {
    CCFs_train <- CCFs_withlands[index,]
    CCFs_test = setdiff(CCFs_withlands, CCFs_train)
    
    rtrees <- randomForest(factor(match) ~ ., data = CCFs_train[,includes], ntree = 300)
    
    CCFs_test$forest <- predict(rtrees, newdata = CCFs_test, type = "prob")[,2]
    imp <- data.frame(importance(rtrees))
    confmat <- as.data.frame(xtabs(~(forest > 0.5) + match + study.x + study.y, data = CCFs_test))
    names(confmat) <- c("Prediction", "Actual", "study.x", "study.y", "Count")
    
    mydf <- confmat %>%
        mutate(Result = rep(c("True Negative", "False Positive", "False Negative", "True Positive"), times = 9),
               Study = paste(study.x, study.y, sep = "_")) %>%
        select(Study, Result, Count) %>%
        group_by(Study) %>%
        filter(any(Count > 0)) %>%
        as.data.frame
    
    return(mydf)
}) 
match_result <- cbind(id = rep(1:10, each = nrow(match_result) / 10), match_result)
match_result_wide <- match_result %>%
    spread(key = Result, value = Count)

match_result %>%
    group_by(Result) %>%
    summarise(Count = mean(Count))
```

The confusion matrix suggests that false negatives are a serious problem. In other words, our algorithm is a bit too conservative in predicting a match when in fact the bullets were fired from the same gun barrel. We can break down the confusion matrix depending on the study that each of the two lands originated from.

```{r}
match_result %>%
    group_by(Study, Result) %>%
    summarise(Count = mean(Count))
```

What we find is that both the rate and the raw number of false negatives is being artificially inflated by comparisons between lands from the Cary Persistence study. We can attempt to quantify the effect of the study on the matching probability by fitting a new random forest which attempts to predict the study based on the derived features. Ideally, if the assumption of independence between lands holds, this forest should have poor performance - The set of derived features should be relatively consistent among known matches and known non-matches regardless of the study.

```{r}
includes_study <- setdiff(names(CCFs_withlands), c("compare_id", "profile1_id", "profile2_id",
                                         "barrel.x", "barrel.y", "match",
                                         "bullet.x", "bullet.y", "land.x", "land.y",
                                         "land_id.x", "land_id.y", "study.x", "study.y"))

result_study <- trainIndex %>% purrr::map_df(function(index) {
    CCFs_train <- CCFs_withlands[index,]
    CCFs_test = setdiff(CCFs_withlands, CCFs_train)
    
    CCFs_train$study <- factor(paste(CCFs_train$study.x, CCFs_train$study.y, sep = "_"))
    CCFs_test$study <- factor(paste(CCFs_test$study.x, CCFs_test$study.y, sep = "_"))
    
    includes_study <- setdiff(names(CCFs_train), c("compare_id", "profile1_id", "profile2_id", "barrel.x", "barrel.y", "match", "bullet.x", "bullet.y", "land.x", "land.y", "land_id.x", "land_id.y", "study.x", "study.y"))

    rtrees_study <- randomForest(study ~ ., data = CCFs_train[,includes_study], ntree = 300)
    
    CCFs_test$study_forest <- predict(rtrees_study, newdata = CCFs_test)
    confmat <- as.data.frame(xtabs(~study_forest + study, data = CCFs_test))
    names(confmat) <- c("Prediction", "Actual", "Count")
    
    return(confmat)
})
```

```{r}
study_result <- cbind(id = rep(1:10, each = nrow(result_study) / 10), result_study)
study_result_wide <- study_result %>%
    spread(key = Actual, value = Count) %>%
    group_by(Prediction) %>%
    summarise_each(funs(mean)) %>%
    select(-id) %>%
    as.data.frame
study_result_wide
```

Figure \ref{fig:ccfstudy} give the distributions of the features defined above, faceted by whether the lands are known to be fired from the same gun barrel, across different study to study comparisons.

```{r ccfstudy, echo=FALSE, message=FALSE, fig.cap="Distribution of the features, facetted by match, for different study to study comparisons of lands.", fig.height=13, fig.width=10}
CCFs_features <- CCFs_withlands %>%
    mutate(study = factor(paste(study.x, study.y, sep = "_"))) %>%
    select(-matches("_id|study.x|study.y|barrel|bullet|land|signature_length|non_cms|sd_D|lag")) %>%
    select(study, match, everything()) %>%
    gather(key = feature, value = value, 3:ncol(.))

ggplot(CCFs_features, aes(x = study, y = value)) +
    geom_boxplot() +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90)) +
    facet_grid(feature~match, scales = "free") +
    ggtitle("Distributions of Features by Study and Match") +
    xlab("Study") +
    ylab("Value")
```
