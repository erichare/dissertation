---
title: "Proposal: Enabling Scientists to Understand their Data using Web-Based Statistical Tools"
author: "Eric Hare"
date: \today{}
knit: "bookdown::render_book"
output:
  bookdown::gitbook:
    lib_dir: "book_assets"
  bookdown::pdf_book:
    keep_tex: yes
bibliography: references.bib
fontsize: 12pt
geometry: margin=1in
header-includes:
    - \usepackage{setspace}
    - \doublespacing
---

## Introduction

The computing revolution has opened up statistical methods and tools to a broad range of fields. With the growing popularity of R in particular, the wide range of choices of open source statistical routines in the form of packages has significantly expanded statistical computing capabilities. Still, there remains a fundamental obstacle to the use of R. Effective use of R requires a commitment to learning and understanding programming, which some individuals, companies, and researchers may not have the time or desire to do. Furthermore, although the open-source nature of R is one of its biggest assets, it also means that there is a more rapid development cycle than would often be found in more corporate software solutions. This means that R developers must continue to maintain their code while learning new programming concepts.

A number of tools have been developed in an attempt to address this issue. The commerical software on which R is derived, S-PLUS, includes a rudimentary graphical user interface (GUI) supporting data editing, graphing, and basic statistics. Over time, GUIs were developed for R as well. One of the first was R Commander, which provides a wrap-around user interface for R. With drop-down menus allowing point-and-click selection of a number of common data analysis and statistical functions, analysis could be performed without a knowledge of programming. More recently, the program Deducer also abstracts the programming into graphical menus and buttons. It expands on R Commander by providing an effective data viewer, help system, and easy to read tables displaying the results.

GUIs have some natural limitations that often make them a less appealing option for researchers. The results of an analysis from a GUI are not typically reproducible. Whereas an R script can be created, shared, and executed elsewhere, the actions taken in a GUI are not transcribed and portable. GUIs also tend to slow down the development and iteration process once the user has become more comfortable with the programming concepts. For instance, scripts allow copying and pasting of code blocks that need only minor modifications. In a GUI, the options representing a code block would need to be individually chosen through drop-down menus.

Recognizing some of these limitations, other approaches have been taken to easing the transition to working with R. RStudio provides a GUI around R with expanded functionality, but maintains focus on the scripting and coding aspect. In this sense, RStudio more readily resembles an IDE (Integrated Development Environment), which aid the programmer rather than attempting to abstract the programming away. While this allows reproducibility and may still help a less experienced programmer begin to get started in a programming language, it still depends on a continuing effort to learn programming.

Essentially, we can partition researchers into three broad sets:

1. Those who have no interest in using statistics to aid their research
2. Those who have interest in using statistics to aid their research, and have a knowledge of or interest in gaining a knowledge of programming
3. Those who have interest in using statistics to aid their research, but have no interest in programming, or significantly struggle in programming

We can hope to change the minds of the people in set 1, moving them into sets 2 or 3. Those in set 2 have most to gain from the use of R and/or RStudio. On the other hand, those in set 3 have the most to gain from effective GUIs that do not require programming knowledge. The Shiny package for R provides a framework for researchers who fall in set 2 to provide a service to those in set 3. Shiny is a web development framework which can help turn the results of an R analysis into an interactive web application. Results can be generated by browsing to the website at which the Shiny application is deployed, and using GUI elements (dropdowns, text boxes, tabs) similar to R Commander or Deducer in order to generate results. But a Shiny application is standard R code, and hence maintains the reproducibility and maintainability benefits of standard R scripts.

Because Shiny offers a solution which maintains the benefits of both GUIs and standard programming, I believe it can form the basis for a new set of tools and concepts that greatly expand the reach of statistics. Those who are comfortable with programming can now provide functionality to those who aren't. This functionality can enable researchers to see, understand, and work with their data in ways that they were simply unable to.

I have focused on three broad areas that can benefit from graphical rather than purely programmatic tools:

1. Forensic Science
2. Statistics Education
3. Visual Inference

Each of these areas, I believe, can benefit significantly from functionality present in R and associated packages. But for the forseeable future, each field will also include a significant number of students, researchers, and scientists that have little-to-no knowledge of R, and will therefore be dissuaded from attempting its use. Working at the interface of Statistics and Computer Science, I will explain the current state of the art in each of these fields, and how a reproducible R and web-based solution can help to address the current shortcomings.

## Forensic Science

XXX Cite all the features, table of features vs from literature XXX

The judicial system would seem to be an especially good candidate for the integration of a statistical approach. In the United States, suspects are considered innocent until proven guilty "beyond a reasonable doubt". This in many ways parallels traditional hypothesis testing approaches, in which a pre-defined cut-off (significance level) is used to determine the threshold at which the null hypothesis is rejected (which presumably should occur once the evidence leads us beyond a reasonable doubt). 

But such probabilistic thinking doesn't always occur. In particular, bullet matching, or the process of determining whether two bullets were fired from the same gun barrel, has been the subject of intense scrutiny in the past 15 years. In 2005, in *United States vs. Green*, the court ruled that the forensic expert in question could not confirm that the bullet casings came from a specific weapon with certainty, but could merely "describe" other casings which are similar. Further court cases in the late 2000s expressed caution about the use of firearms identification evidence [@giannelli:2011]. This scrutiny culminated in the 2009 National Academy of Sciences report questioning the scientific validity of many forensic methods including firearm examination. The report states that "[m]uch forensic evidence -- including, for example, bite marks and firearm and toolmark identification is introduced in criminal trials without any meaningful scientific validation, determination of error rates, or reliability testing to explain the limits of the discipline" [@NAS:2009].

Rifling, manufacturing defects, and impurities in a barrel create striation marks on the bullet during the firing process. These marks are assumed to be unique to the barrel, as described in a 1992 AFTE article [@afte:1992]. Current standard practice for bullet matching relies in part on the assessment of the so-called maximum number of consecutively matching striae (CMS), first defined by @biasotti:1959. One of the primary issues with this procedure is that a human inspection to determine CMS is subjective [@miller:1998]. Human inspection also requires on-site analysis of the bullets, which can be costly and time-consuming, and introduces the potential for differing opinions across different forensic examiners.

A modern development in this realm is the adoption of an open format for storing 3D topographical images of bullets in a format called x3p (XML 3-D Surface Profile). The x3p format conforms to the ISO5436-2 standard\footnote{\url{http://sourceforge.net/p/open-gps/mwiki/X3p/}}, implemented to provide a simple and standard conforming way to exchange 2D and 3D profile data. It was adopted by the OpenFMC (Open Forensic Metrology Consortium\footnote{\url{http://www.openfmc.org/}}), a group of academic, industry, and government firearm forensics researchers whose aim is to establish best practices for researchers using metrology in forensic science. Furthermore, NIST (the National Institute for Standards and Technology) is developing a database to allow searching and downloading of these x3p files\footnote{\url{https://tsapps.nist.gov/NRBTD/}}. Although limited to around 70 bullets at the time of this writing, this database in conjuction with open-source software to work with .x3p files opens up a whole new set of possibilities in terms of a statistical foundation for bullet matching.

The feasibility of creating a database of ballistic images that could be used to identify guns used in crimes was evaluated in a 2008 report by the National Research Council [@nap:2008]. The evaluation investigated the scalability of NIBIN (National Integrated Ballistic Information Network), which uses proprietary matching algorithms provided by IBIS. The bottom line of the report was that in spite of the many technical and practical hurdles, solutions to all but one problem could be found. The problem that remained is that statistically, the quality of the matching algorithm (in this case, of breech-face marks and firing pin impressions) could not withstand a hugely increased number of records while still maintaining a reasonable workload for forensic examiners, who have to examine possible matches suggested by the system.

With this in mind, I'm proposing a modern bullet matching framework, written in R, which uses open and transparent statistical techniques to arrive at a predicted probability of a match at the bullet land level. This framework will be provided as an R package with an associated web frontend. The web frontend will allow forensic examiners to upload bullet land images, examine the surface topologies, and perform each aspect of the algorithm in order to arrive at a probability of a match. The final display will include a results page in which all chosen parameters of the algorithm are provided so that a report on the results can be pretended and cross-checked by other researchers. Figure \@ref(fig:bullets-app). displays a prototype user interface for this algorithm.

```{r bullets-app, echo=FALSE, fig.cap='Prototype user interface for the bullet matching algorithm.'}
knitr::include_graphics(rep("images/bullets_app.png"))
```

## Statistics Education

The widespread adoption of R [@r-stat] as a tool for statistical analysis has undoubtedly been an important development for the scientific community. However, using R in most cases still requires a basic knowledge of programming concepts which may pose a steep learning curve for the introductory statistics student [@5359977]. This additional time commitment may explain why introductory courses often utilize point-and-click applications, even if the instructor himself/herself uses R in their own work. Still, some compromises must be made when using many graphical applications, including dealing with software licenses and unsupported desktop platforms.

Multiple software packages have recently been written in an attempt to spur interest in  R programming and statistics. DataCamp's [@datacamp] courses are user-friendly ways to learning basic R programming and data analysis techniques. Swirl [@swirl] is a similar interactive tool to make learning R more fun by learning it within R itself. Project MOSAIC [@mosaic] has created a suite of tools to simplify the teaching of statistics in the form of an R package. The primary goal of DataCamp and Swirl is to teach R programming, rather than facilitate the learning of introductory statistics. Project MOSAIC's goal is to faciliate this learning, but using the package requires a knowledge of R programming that the introductory student may not have. R Commander [@fox2005] and Deducer [@fellows2012] provide a graphical front-end to many statistical functions in R, but are not web-based and thus require local installation and configuration. iNZight Lite [@inzight] also attempts to expose students to data analysis without requiring programming knowledge, but does not include reproducible R code, and therefore has less of a focus on spurring interest in coding for students.

Upon the release of RStudio's Shiny [@shiny] it became easier for an R-based analysis to be converted to an interactive web application. Shiny helps generate modern, fluid web applications which scale to different device sizes, requiring only a browser to access. Because of this, it targets some of the issues of pre-existing statistical software. Software written in Shiny requires no installation, no licenses, and no knowledge of programming. With that in mind, a modern statistical software program aimed at supplementing an introductory statistics class could prove very useful. We call this software **intRo**. Figure \@ref(fig:intro-app). displays the default **intRo** user interface.

```{r intro-app, echo=FALSE, fig.cap='Default user interface for the intRo application.'}
knitr::include_graphics(rep("images/intro_app.png"))
```

There are two primary guiding principles behind the design of **intRo**. The first is modularity. For software to maintain simplicity, it should not overload the user with options. But for software to remain flexible enough to be used by a wide variety of universities, options must be available. We've addressed this seeming contradiction by building each piece of functionality as a *module*. Modules are self-contained sets of R scripts which define the user interface, the functionality, and the output of a particular set of statistical functions in **intRo**. Modules can be dynamically included or removed at runtime by an instructor, presenting to the user a minimal set of options relevant to their particular classroom setting.

The second guiding principle is reproducibility. While we wrote **intRo** to be accessible to users of all computer skill levels, we still recognize that there is significant value in programming. In particular, R scripts can be shared, reproduced, and the results regenerated. We built in these reproducibility concepts to **intRo**. The results generated in an **intRo** session are automatically converted into reproducible R code. Each point-and-click action is transcribed, and the results visible in **intRo** will match the results produced by the R code at the bottom of the page. This overcomes a common limitation of GUIs in that typically results are not reproducible. XXX There does exist a caveat to this reproducibility, however. **intRo** makes use of some basic interactive graphics routines through the incorporation of the ggvis  package [@ggvis]. In particular, hovering on points in a scatterplot will produce tooltips indicating the exact values of $x$ and $y$ in the plot. However, this hovering action is not reproduced in code. Some recent research is being done in this area [e.g., @stevens2010holoviews], but **intRo** is not currently utilizing these methods. XXX

While **intRo** has been stable in development for some time, there is some major work that must be done in order to see a wider adoption. In particular, the procedure for creating and deploying new **intRo** modules is currently a black box. It is a bit cumbersome, and some streamlining can be done in order to make it both easier and faster to do. There is also a rather limited set of functionality available at this stage. The lack of functionality can be remedied by creating an official repository of modules, which would also serve as examples for developers wishing to create their own modules.

## Visual Inference

XXX Lay out EXACTLY what visual inference is, pull out type I, type II, power. Cite the papers where its shown this really works. Thorough introduction from the basics. Hypothesis testing 101 (P-Value, power, errors, etc). Talk about right decisions for the wrong reason. (Which is why we ask). Rather than cite dissertation, cite Arxiv paper. XXX

The lineup protocol is an inferential framework which acts as an exploratory data analysis corrolary to traditional hypothesis testing [@buja2009statistical]. This technique was extensively explored by @majumder2013. As part of this work, a web framework was developed in order to conduct lineup studies on Amazon Mechanical Turk. This framework has been a significant development in visual inference research, enabling experiments such as XXX (list of experiments) XXX.

I propose a modernization of this framework which allows researchers to more seamlessly deploy studies, while automatically deriving the distribution of the test statistic of interest based on the blocking factors of the lineups used. A prototype user interface is shown in Figure \@ref(fig:lineups-app). displays a prototype user interface.

```{r lineups-app, echo=FALSE, fig.cap='Prototype user interface for the lineups web application.'}
knitr::include_graphics(rep("images/lineups_app.png"))
```

## Conclusion

## References
