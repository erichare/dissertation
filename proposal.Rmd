---
title: "Proposal: Enabling Scientists to Understand their Data using Web-Based Statistical Tools"
author: "Eric Hare"
date: \today{}
knit: "bookdown::render_book"
output:
  bookdown::pdf_book:
    keep_tex: yes
bibliography: references.bib
fontsize: 12pt
geometry: margin=1in
header-includes:
    - \usepackage{setspace}
    - \usepackage{pdfpages}
    - \usepackage{float}
    - \doublespacing
---

## Introduction

The computing revolution has opened up statistical methods and tools to a broad range of fields. With the growing popularity of R [@R] in particular, the wide range of choices of open source statistical routines in the form of packages has significantly expanded statistical computing capabilities. Still, there remains a fundamental obstacle to the use of R. Effective use of R requires a commitment to learning and understanding programming, which some individuals, companies, and researchers may not have the time or desire to do. Furthermore, although the open-source nature of R is one of its biggest assets, it also means that there is a more rapid development cycle than would often be found in more corporate software solutions. This means that R developers must continue to maintain their code while learning new programming concepts.

A number of tools have been developed in an attempt to address this issue. The commerical software on which R is derived, S-PLUS [@SPlus], includes a rudimentary graphical user interface (GUI) supporting data editing, graphing, and basic statistics. Over time, GUIs were developed for R as well. One of the first was R Commander [@fox2005], which provides a wrap-around user interface for R. With drop-down menus allowing point-and-click selection of a number of common data analysis and statistical functions, analysis could be performed without a knowledge of programming. More recently, the program Deducer [@fellows2012] also abstracts the programming into graphical menus and buttons. It expands on R Commander by providing an effective data viewer, help system, and easy to read tables displaying the results.

GUIs have some natural limitations that often make them a less appealing option for researchers. The results of an analysis from a GUI are not typically reproducible. Whereas an R script can be created, shared, and executed elsewhere, the actions taken in a GUI are not transcribed and portable. GUIs also tend to slow down the development and iteration process once the user has become more comfortable with the programming concepts. For instance, scripts allow copying and pasting of code blocks that need only minor modifications. In a GUI, the options representing a code block would need to be individually chosen through drop-down menus.

Recognizing some of these limitations, other approaches have been taken to easing the transition to working with R. RStudio [@RStudio] provides a GUI around R with expanded functionality, but maintains focus on the scripting and coding aspect. In this sense, RStudio more readily resembles an IDE (Integrated Development Environment), which aid the programmer rather than attempting to abstract the programming away. While this allows reproducibility and may still help a less experienced programmer begin to get started in a programming language, it still depends on a continuing effort to learn programming.

Essentially, we can partition researchers into three broad sets:

1. Those who have no interest in using statistics to aid their research
2. Those who have interest in using statistics to aid their research, and have a knowledge of or interest in gaining a knowledge of programming
3. Those who have interest in using statistics to aid their research, but have no interest in programming, or significantly struggle in programming

We can hope to change the minds of the people in set 1, moving them into sets 2 or 3. Those in set 2 have most to gain from the use of R and/or RStudio. On the other hand, those in set 3 have the most to gain from effective GUIs that do not require programming knowledge. The Shiny package for R provides a framework for researchers who fall in set 2 to provide a service to those in set 3. Shiny [@shiny] is a web development framework which can help turn the results of an R analysis into an interactive web application. Results can be generated by browsing to the website at which the Shiny application is deployed, and using GUI elements (dropdowns, text boxes, tabs) similar to R Commander or Deducer in order to generate results. But a Shiny application is standard R code, and hence maintains the reproducibility and maintainability benefits of standard R scripts.

Because Shiny offers a solution which maintains the benefits of both GUIs and standard programming, I believe it can form the basis for a new set of tools and concepts that greatly expand the reach of statistics. Those who are comfortable with programming can now provide functionality to those who aren't. This functionality can enable researchers to see, understand, and work with their data in ways that they were simply unable to.

I have focused on three broad areas that can benefit from graphical rather than purely programmatic tools:

1. Forensic Science
2. Statistics Education
3. Visual Inference

Each of these areas, I believe, can benefit significantly from functionality present in R and associated packages. But for the forseeable future, each field will also include a significant number of students, researchers, and scientists that have little-to-no knowledge of R, and will therefore be dissuaded from attempting its use. Working at the interface of Statistics and Computer Science, I will explain the current state of the art in each of these fields, and how a reproducible R and web-based solution can help to address the current shortcomings.

## Forensic Science

The judicial system would seem to be an especially good candidate for the integration of a statistical approach. In the United States, suspects are considered innocent until proven guilty "beyond a reasonable doubt". This in many ways parallels traditional hypothesis testing approaches, in which a pre-defined cut-off (significance level) is used to determine the threshold at which the null hypothesis is rejected (which presumably should occur once the evidence leads us beyond a reasonable doubt). 

But such probabilistic thinking doesn't always occur. In particular, bullet matching, or the process of determining whether two bullets were fired from the same gun barrel, has been the subject of intense scrutiny in the past 15 years. In 2005, in *United States vs. Green*, the court ruled that the forensic expert in question could not confirm that the bullet casings came from a specific weapon with certainty, but could merely "describe" other casings which are similar. Further court cases in the late 2000s expressed caution about the use of firearms identification evidence [@giannelli:2011]. This scrutiny culminated in the 2009 National Academy of Sciences report questioning the scientific validity of many forensic methods including firearm examination. The report states that "[m]uch forensic evidence -- including, for example, bite marks and firearm and toolmark identification is introduced in criminal trials without any meaningful scientific validation, determination of error rates, or reliability testing to explain the limits of the discipline" [@NAS:2009].

Rifling, manufacturing defects, and impurities in a barrel create striation marks on the bullet during the firing process. These marks are assumed to be unique to the barrel, as described in a 1992 AFTE article [@afte:1992]. Current standard practice for bullet matching relies in part on the assessment of the so-called maximum number of consecutively matching striae (CMS), first defined by @biasotti:1959. One of the primary issues with this procedure is that a human inspection to determine CMS is subjective [@miller:1998]. Human inspection also requires on-site analysis of the bullets, which can be costly and time-consuming, and introduces the potential for differing opinions across different forensic examiners.

A modern development in this realm is the adoption of an open format for storing 3D topographical images of bullets in a format called x3p (XML 3-D Surface Profile). The x3p format conforms to the ISO5436-2 standard\footnote{\url{http://sourceforge.net/p/open-gps/mwiki/X3p/}}, implemented to provide a simple and standard conforming way to exchange 2D and 3D profile data. It was adopted by the OpenFMC (Open Forensic Metrology Consortium\footnote{\url{http://www.openfmc.org/}}), a group of academic, industry, and government firearm forensics researchers whose aim is to establish best practices for researchers using metrology in forensic science. Furthermore, NIST (the National Institute for Standards and Technology) is developing a database to allow searching and downloading of these x3p files\footnote{\url{https://tsapps.nist.gov/NRBTD/}}. Although limited to around 70 bullets at the time of this writing, this database in conjuction with open-source software to work with .x3p files opens up a whole new set of possibilities in terms of a statistical foundation for bullet matching.

The feasibility of creating a database of ballistic images that could be used to identify guns used in crimes was evaluated in a 2008 report by the National Research Council [@nap:2008]. The evaluation investigated the scalability of NIBIN (National Integrated Ballistic Information Network), which uses proprietary matching algorithms provided by IBIS. The bottom line of the report was that in spite of the many technical and practical hurdles, solutions to all but one problem could be found. The problem that remained is that statistically, the quality of the matching algorithm (in this case, of breech-face marks and firing pin impressions) could not withstand a hugely increased number of records while still maintaining a reasonable workload for forensic examiners, who have to examine possible matches suggested by the system.

With this in mind, we have developed an automated matching routine, written in R, which uses open and transparent statistical techniques to arrive at a predicted probability of a match at the bullet land level. This framework is provided as an R package called x3prplus [@x3prplus] with an associated web frontend. The web frontend allows forensic examiners to upload bullet land images, examine the surface topologies, and perform each aspect of the algorithm in order to arrive at a probability of a match. The final display includes a results page in which all chosen parameters of the algorithm are provided so that a report on the results can be pretended and cross-checked by other researchers. Figure \@ref(fig:bullets-app). displays a prototype user interface for this algorithm.

```{r bullets-app, echo=FALSE, fig.cap='Prototype user interface for the bullet matching algorithm.', fig.pos='H'}
knitr::include_graphics("images/bullets_app.png")
```

This will not be the first automatic system [@xie:2009, @riva:2014, @bachrach:2002]. But it will be the first that uses a publicly accessible database, includes fully reproducible results, and uses a broad set of derived features to produce probabilities or scores based on a machine learning algorithm. Critical to the success of this algorithm will be the extraction of this set of features describing a bullet signature. In addition to the aforementioned CMS, the CCF will be used, as it has for other bullet matching applications [@vorburger:2011]. Traditional bullet matching methods have used strict cutoffs (for instance, 6 CMS) to determine a match versus a non-match. We are aiming to be more robust in using a number of features and deriving conditional probabilities of matches given particular values of these features.

* *CCF*: Function of the optimum shift distance measuring the correlation between two profiles [@vorburger:2011]
* *CMS*: Striated markings that line up exactly with one another without a break or dissimilarity in between them [@biasotti:1959, @thompson:2013]. This and other forensic science papers using CMS typically count a single peak as a striae, while we count peaks and valleys, so our definition typically yields CMS values about twice those commonly found in the literature.
* *CMNS*: Striated markings that do not line up exactly with another, without matching striation between them.
* *Matches*: The number of matches between two signatures
* *Non-Matches*: The number of matches between two signatures
* *D* = $\sqrt{\frac{1}{\text{\#}t}\sum_t \left[f(t) - g(t)\right]^2}$ where $f(t)$ and $g(t)$ are aligned signatures. The euclidean vertical distance between surface measurements of aligned signatures. This is a measure of the total variation between two profiles [@clarkson1933definitions].
* *S*: The sum $S$ of average absolute heights of matched extrema: for each of the two matched stria, compute the average of the absolute heights of the peaks or valleys. $S$ is then defined as the sum of all these averages.  

This work has been submitted and accepted (with revisions) by the Annals of Applied Statistics. We are following up by investigating the properties of different features as applied to degraded bullets, and when compared with common cut-offs for match, non-match, and inconclusive from the literature.

## Statistics Education

The widespread adoption of R [@r-stat] as a tool for statistical analysis has undoubtedly been an important development for the scientific community. However, using R in most cases still requires a basic knowledge of programming concepts which may pose a steep learning curve for the introductory statistics student [@5359977]. This additional time commitment may explain why introductory courses often utilize point-and-click applications, even if the instructor himself/herself uses R in their own work. Still, some compromises must be made when using many graphical applications, including dealing with software licenses and unsupported desktop platforms.

Multiple software packages have recently been written in an attempt to spur interest in R programming and statistics. DataCamp's [@datacamp] courses are user-friendly ways to learning basic R programming and data analysis techniques. Swirl [@swirl] is a similar interactive tool to make learning R more fun by learning it within R itself. Project MOSAIC [@mosaic] has created a suite of tools to simplify the teaching of statistics in the form of an R package. The primary goal of DataCamp and Swirl is to teach R programming, rather than facilitate the learning of introductory statistics. Project MOSAIC's goal is to faciliate this learning, but using the package requires a knowledge of R programming that the introductory student may not have. R Commander [@fox2005] and Deducer [@fellows2012] provide a graphical front-end to many statistical functions in R, but are not web-based and thus require local installation and configuration. iNZight Lite [@inzight] also attempts to expose students to data analysis without requiring programming knowledge, but does not include reproducible R code, and therefore has less of a focus on spurring interest in coding for students.

Upon the release of RStudio's Shiny [@shiny] it became easier for an R-based analysis to be converted to an interactive web application. Shiny helps generate modern, fluid web applications which scale to different device sizes, requiring only a browser to access. Because of this, it targets some of the issues of pre-existing statistical software. Software written in Shiny requires no installation, no licenses, and no knowledge of programming. With that in mind, a modern statistical software program aimed at supplementing an introductory statistics class could prove very useful. We call this software **intRo**. Figure \@ref(fig:intro-app). displays the default **intRo** user interface.

```{r intro-app, echo=FALSE, fig.cap='Default user interface for the intRo application.', fig.pos='H'}
knitr::include_graphics(rep("images/intro_app.png"))
```

There are two primary guiding principles behind the design of **intRo**. The first is modularity. For software to maintain simplicity, it should not overload the user with options. But for software to remain flexible enough to be used by a wide variety of universities, options must be available. We've addressed this seeming contradiction by building each piece of functionality as a *module*. Modules are self-contained sets of R scripts which define the user interface, the functionality, and the output of a particular set of statistical functions in **intRo**. Modules can be dynamically included or removed at runtime by an instructor, presenting to the user a minimal set of options relevant to their particular classroom setting.

The second guiding principle is reproducibility. While we wrote **intRo** to be accessible to users of all computer skill levels, we still recognize that there is significant value in programming. In particular, R scripts can be shared, reproduced, and the results regenerated. We built in these reproducibility concepts to **intRo**. The results generated in an **intRo** session are automatically converted into reproducible R code. Each point-and-click action is transcribed, and the results visible in **intRo** will match the results produced by the R code at the bottom of the page. This overcomes a common limitation of GUIs in that typically results are not reproducible. There does exist a caveat to this reproducibility, however. **intRo** makes use of some basic interactive graphics routines through the incorporation of the ggvis  package [@ggvis]. In particular, hovering on points in a scatterplot will produce tooltips indicating the exact values of $x$ and $y$ in the plot. However, this hovering action is not reproduced in code. Some recent research is being done in this area [e.g., @stevens2010holoviews], but **intRo** is not currently utilizing these methods.

While **intRo** has been stable in development for some time, there is some major work that must be done in order to see a wider adoption. In particular, the procedure for creating and deploying new **intRo** modules is currently a black box. It is a bit cumbersome, and some streamlining can be done in order to make it both easier and faster to do. There is also a rather limited set of functionality available at this stage. The lack of functionality can be remedied by creating an official repository of modules, which would also serve as examples for developers wishing to create their own modules.

This work has been submitted and accepted (with revisions) by the Journal of Computational and Graphical Statistics. In this paper, we focus on intRo as a case study in the development of a modular, extensible, and web-based statistical software application, which allows the results generated to be seamlessly reproduced in R code.

## Visual Inference

A major aspect of exploratory data analysis is producing graphical displays, which help draw attention to patterns in data. Graphical displays are even used for model assessment [@Gelman:2004gg]. Ultimately, a graphical display is a statistic, or some function of the data. Unlike a traditional hypothesis test setting, however, a graphical display does not have a clearly defined reference distribution from which comparisons can be made (contrast this with, for instance, a statistic such as the sample mean for large samples which is asymptotically normally distributed under regularity conditions) This makes the identification of "significant" features of a graphical display far more challenging.

In a traditional hypothesis test, a relevant test statistic is stated and its distribution is derived under an assumed given null hypothesis. Typically, a significance level $\alpha$ for the test is selected, often $.05$. If the probability of obtaining a test statistic under the null hypothesis as or more extreme than the observed test statistic from the data is less than $\alpha$, we conclude that the null hypothesis should be rejected. Note that we are directly fixing a significance level, and thus fixing the Type I Error, or the probability of rejecting the null hypothesis given that the null hypothesis was in fact true. Of practical importance is the power of the test. The power is the probability that we will reject the null hypothesis given that it is false, as we should. Taking the value of the power as $p$, $1 - p$ will yield the Type II Error rate, the probability of failing to reject the null hypothesis when in fact the null hypothesis is false.

The lineup protocol is an inferential framework which acts as an exploratory data analysis corrolary to traditional hypothesis testing [@buja2009statistical]. The key idea is recognizing the parallel between discoveries in a graphical display and rejection of a null hypothesis in a traditional hypothesis test. In the universe of possible discoveries which are not pre-specified, one that is identified is a test statistic that resulted in rejection. The concept extends to null plots, which represent the visual inference corrolary to null distributions in traditional hypothesis testing. Null plots are a possibly infinite set of displays that are randomly generated by sampling from the null hypothesis. By placing a target plot within a set of $m$ null plots and asking observers to identify the "most different" plot, a visual inference test can be conducted. As in the hypothesis testing framework, a significance level $\alpha$ is typically pre-specified. Note however that this cannot be controlled exactly because of the discrete number of plots shown in a lineup [@Majumder:2013ie]. By increasing the number of observers, however, this can be made arbitrarily small. Power and Type II Error can also be computed, but because observers have differing levels of ability, the observer skill must be estimated by aggregating the overall results across a number of lineups from each observer. One interesting aspect of visual inference when compared with traditional hypothesis testing is the case of an observer identifying the correct plot for the wrong reason. In this scenario, the aforementioned "discovery" doesn't correspond to the feature being tested. This is why visual inference tests ask for the reason an observer selected a particular display.

This protocol has been applied to a wide variety of situations. It has been used in order to identify the features of Q-Q plots which are most relevant, in particular to show that in certain cases detrended Q-Q plots are more powerful than their trended counterparts [@Loy:2016dqa]. Lineups have been shown to compare favorably with more traditional tests in the context of a linear model as well. @Majumder:2013ie demonstrated this, as well as showing that there are cases in which visual tests yield higher power than traditional tests, such as when the effect size is large. Lineups have also been used to assess the power of competing graphical designs [@Hofmann:tvcg:12].

As part of this work, a web framework was developed in order to conduct lineup studies on Amazon Mechanical Turk [@majumder2013]. This framework has been a significant development in visual inference research, enabling experiments such as XXX (list of experiments) XXX. We have begun the development of a modernization of this framework which allows researchers to more seamlessly deploy studies, while automatically deriving the distribution of the test statistic of interest based on the blocking factors of the lineups used. A prototype user interface is shown in Figure \@ref(fig:lineups-app).

```{r lineups-app, echo=FALSE, fig.cap='Prototype user interface for the lineups web application.', fig.pos='H'}
knitr::include_graphics(rep("images/lineups_app.png"))
```

## Bullets Paper

The following is the current revision of the bullets paper, submitted and accepted by the Annals of Applied Statistics.

\includepdf[pages=-]{imaging-paper.pdf}

## intRo Paper

The following is the current revision of the intRo paper, submitted and accepted by the Journal of Computational and Graphical Statistics.

\includepdf[pages=-]{paper.pdf}

## References
